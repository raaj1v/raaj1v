{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rajeev.mishra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "from datetime import datetime\n",
    "from fastapi import Request\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from preprocessing import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator MultiLabelBinarizer from version 1.2.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "app = FastAPI()\n",
    "\n",
    "model=load_model(r\"model/cleandata_basic_30L_2.h5\")\n",
    "multilabel_binarizer = joblib.load(r\"classes/cleandata_Updated_mlb_All_Product_2.pkl\")\n",
    "labels = multilabel_binarizer.classes_\n",
    "tokenizer = joblib.load(r\"tokenizer/cleandata_Updated_tokenizer_30L_2.pkl\")\n",
    "full_form_data=pd.read_csv(r\"shortCodesProduct.csv\")\n",
    "full_form_data=full_form_data.applymap(lambda x: x.lower() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def log_predictions(input_sentence, predictions):\n",
    "    log_file = \"prediction_log.csv\"\n",
    "    file_exists = os.path.exists(log_file)\n",
    "\n",
    "    with open(log_file, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        if not file_exists:\n",
    "            writer.writerow([\"Date Time\", \"IP Address\", \"Input Sentence\", \"Predicted Labels\"])\n",
    "\n",
    "        date_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        # ip_address = request.client.host\n",
    "\n",
    "        writer.writerow([date_time, input_sentence, predictions])\n",
    "\n",
    "def predict_op_th_main(new_sentence, threshold):\n",
    "    new_sentence=new_sentence.lower()\n",
    "    new_sentence=replace_short_with_full(new_sentence, full_form_data)\n",
    "    new_sentence = cleanText(new_sentence)\n",
    "    new_sentence = preprocess_text(new_sentence)\n",
    "    new_sentence_sequences = tokenizer.texts_to_sequences([new_sentence])\n",
    "    new_sentence_padded = pad_sequences(new_sentence_sequences, maxlen=512)\n",
    "    predicted_domain = model.predict(new_sentence_padded)\n",
    "    # result = [{label,  float(prob)} for label, prob in zip(labels, predicted_domain[0]) if prob >= threshold]\n",
    "    result = [{label} for label, prob in zip(labels, predicted_domain[0]) if prob >= threshold]\n",
    "\n",
    "    print(type(result))\n",
    "    return result\n",
    "\n",
    "def Predict_Labels(new_sentence):\n",
    "    threshold = 0.3\n",
    "    output = predict_op_th_main(new_sentence, threshold)\n",
    "    if not output:\n",
    "        threshold = 0.15\n",
    "        output = predict_op_th_main(new_sentence, threshold)\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 256ms/step\n",
      "<class 'list'>\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'automobile ancillaries'}, {'tool'}, {'vehicle'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Predict_Labels(\"supply of tata punch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\lib\\site-packages\\sklearn\\base.py:299: UserWarning: Trying to unpickle estimator MultiLabelBinarizer from version 1.2.2 when using version 1.2.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'shap' has no attribute 'supports_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mshap\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Load your machine learning model (e.g., replace with your actual model loading code)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[39m# Check if the model is supported by SHAP for explanation\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m is_supported \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39;49msupports_model(model)\n\u001b[0;32m      9\u001b[0m \u001b[39mif\u001b[39;00m is_supported:\n\u001b[0;32m     10\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mModel is supported by SHAP for explanation.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'shap' has no attribute 'supports_model'"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "# Load your machine learning model (e.g., replace with your actual model loading code)\n",
    "\n",
    "\n",
    "# Check if the model is supported by SHAP for explanation\n",
    "is_supported = shap.supports_model(model)\n",
    "\n",
    "if is_supported:\n",
    "    print(\"Model is supported by SHAP for explanation.\")\n",
    "else:\n",
    "    print(\"Model is not supported by SHAP for explanation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentence(sentence):\n",
    "    output = Predict_Labels(sentence)\n",
    "    log_predictions(sentence, output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "<class 'list'>\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'building'}, {'civil work'}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentence(\"purani nagarpalika shopping complex dukan kramank 06 any pichda varg aarakchit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_sentence(request: Request, data: InputData):\n",
    "def predict_sentence(data: InputData):\n",
    "    output = Predict_Labels(data.sentence)\n",
    "    log_predictions(data.sentence, output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputData(BaseModel):\n",
    "    sentence: str\n",
    "\n",
    "@app.post(\"/prediction3\")\n",
    "def predict_sentence(request: Request, data: InputData):\n",
    "    output = Predict_Labels(data.sentence)\n",
    "    log_predictions(request, data.sentence, output)\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('prediction_global_data_top10000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          srno      tcno                                       tendersBrief  \\\n",
      "0     70164532  59513614  Digitization Of The \"wille\" Memorial, Lot 1: P...   \n",
      "1     70164533  59513615  Procurement Of Office Supplies For Periodical ...   \n",
      "2     70164534  59513616  Awarding Of Scholarships For Professionally Qu...   \n",
      "3     70164535  59513617  Supply And Delivery Of Materials For Construct...   \n",
      "4     70164536  59513618  Demolition Work - Partial Demolition Of Gollno...   \n",
      "...        ...       ...                                                ...   \n",
      "9853  53344712  51097972      Provision For Procurement Of Air Conditioning   \n",
      "9854  53344401  51097661  Delivery Of Servers And Provision Of Cable Con...   \n",
      "9855  53344408  51097668  Services Of Plugging, Drilling, Re-drilling An...   \n",
      "9856  53344410  51097670  Delivery And Maintenance Of Network Equipment ...   \n",
      "9857  53344411  51097671  Consulting Services For The Assessment Of The ...   \n",
      "\n",
      "                                            productname  \n",
      "0     Computerization,Data Digitization,Digital Scan...  \n",
      "1     Fire Alarm Panel,Fire Alarm System,Fire Brigad...  \n",
      "2     Billeting Service,Concierge Service,Hospitalit...  \n",
      "3     Expressway,RCC road,Road Concreting,Road devel...  \n",
      "4                                            Demolition  \n",
      "...                                                 ...  \n",
      "9853  AC Unit Hiring,Air Condition Plant,Air conditi...  \n",
      "9854  Computer Server,Exchange Server,High End Serve...  \n",
      "9855                     Bore Well,Deep well,Water Well  \n",
      "9856  Adapalene Gel,Adrenaline,Antihypertensive,Anti...  \n",
      "9857                              Customer Satisfaction  \n",
      "\n",
      "[9858 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df2 = pd.read_excel('GloalTendersPrediction.xlsx')\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2['predictions'] = df2['tendersBrief'].apply(Predict_Labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1655069604.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[48], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    curl -X POST \"http://192.168.50.247:8001/prediction3\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d '{\"text\": \"purani nagarpalika shopping complex dukan kramank 06 any pichda varg aarakchit\"}'\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "curl -X POST \"http://192.168.50.247:8001/prediction3\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d '{\"text\": \"purani nagarpalika shopping complex dukan kramank 06 any pichda varg aarakchit\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting EXPLAINABLE AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load pre-trained deep learning text classification model (e.g., TensorFlow/Keras model)\n",
    "model = tf.keras.models.load_model(r'model/cleandata_basic_30L_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load necessary files for interpretation (e.g., vocabulary, tokenizer, etc.)\n",
    "# Replace with actual paths and loading methods for your use case\n",
    "vocabulary = np.load('vocabulary.npy', allow_pickle=True).item()\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=len(vocabulary))\n",
    "tokenizer.word_index = vocabulary\n",
    "\n",
    "# Load NLTK stopwords for text preprocessing\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load SHAP explainer\n",
    "explainer = shap.Explainer(model, tokenizer)\n",
    "\n",
    "# Example sentence for interpretation\n",
    "sentence = \"This is a great product that works well.\"\n",
    "\n",
    "# Preprocess the sentence for prediction and interpretation\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "preprocessed_sentence = preprocess_text(sentence)\n",
    "input_data = tokenizer.texts_to_sequences([preprocessed_sentence])\n",
    "input_data = tf.keras.preprocessing.sequence.pad_sequences(input_data, maxlen=50)\n",
    "\n",
    "# Get SHAP explanations for the prediction\n",
    "shap_values = explainer([input_data])\n",
    "\n",
    "# Summarize the impact of words/tokens on the prediction\n",
    "print(\"SHAP Values for Each Token:\")\n",
    "for token, shap_value in zip(preprocessed_sentence.split(), shap_values[0][0]):\n",
    "    print(f\"Token: {token}, SHAP Value: {shap_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import shap\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load pre-trained deep learning text classification model (e.g., TensorFlow/Keras model)\n",
    "model = tf.keras.models.load_model(r'model/cleandata_basic_30L_2.h5')\n",
    "\n",
    "# Load tokenizer\n",
    "with open(r'tokenizer/cleandata_Updated_tokenizer_30L_2.pkl', 'rb') as tokenizer_file:\n",
    "    tokenizer = pickle.load(tokenizer_file)\n",
    "\n",
    "# Load MultiLabelBinarizer classes\n",
    "with open(r'classes/cleandata_Updated_mlb_All_Product_2.pkl', 'rb') as mlb_classes_file:\n",
    "    mlb_classes = pickle.load(mlb_classes_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rajeev.mishra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Load NLTK stopwords for text preprocessing\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load SHAP explainer\n",
    "explainer = shap.Explainer(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This product is well designed and easy to use.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_sentence = preprocess_text(sentence)\n",
    "input_data = tokenizer.texts_to_sequences([preprocessed_sentence])\n",
    "input_data = tf.keras.preprocessing.sequence.pad_sequences(input_data, maxlen=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Load NLTK stopwords for text preprocessing\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m nltk\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mstopwords\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m stop_words \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords\u001b[39m.\u001b[39mwords(\u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m      5\u001b[0m \u001b[39m# Load SHAP explainer\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "# Load NLTK stopwords for text preprocessing\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load SHAP explainer\n",
    "explainer = shap.Explainer(model, tokenizer)\n",
    "\n",
    "# Example sentence for interpretation\n",
    "sentence = \"This product is well designed and easy to use.\"\n",
    "\n",
    "# Preprocess the sentence for prediction and interpretation\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "preprocessed_sentence = preprocess_text(sentence)\n",
    "input_data = tokenizer.texts_to_sequences([preprocessed_sentence])\n",
    "input_data = tf.keras.preprocessing.sequence.pad_sequences(input_data, maxlen=50)\n",
    "\n",
    "# Get SHAP explanations for the prediction\n",
    "shap_values = explainer([input_data])\n",
    "\n",
    "# Summarize the impact of words/tokens on the prediction\n",
    "print(\"SHAP Values for Each Token:\")\n",
    "for token, shap_value in zip(preprocessed_sentence.split(), shap_values[0][0]):\n",
    "    print(f\"Token: {token}, SHAP Value: {shap_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_compute_main_effects',\n",
       " '_instantiated_load',\n",
       " 'explain_row',\n",
       " 'load',\n",
       " 'save',\n",
       " 'supports_model_with_masker']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(shap.Explainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_text(sentence: str, threshold: float = 0.1):\n",
    "    result = predict_op_th_main(sentence, threshold)\n",
    "    \n",
    "    # Initialize SHAP explainer\n",
    "    explainer = shap.Explainer(predict_op_th_main, tokenizer, max_evals = 2000)\n",
    "    \n",
    "    # Preprocess the sentence for SHAP interpretation\n",
    "    preprocessed_sentence = preprocess_text(sentence)\n",
    "    input_data = tokenizer.texts_to_sequences([preprocessed_sentence])\n",
    "    input_data_padded = pad_sequences(input_data, maxlen=512)\n",
    "    \n",
    "    # Calculate SHAP values\n",
    "    shap_values = explainer(input_data_padded)\n",
    "    \n",
    "    # Summarize SHAP values for each label\n",
    "    shap_summary = {}\n",
    "    for idx, label in enumerate(labels):\n",
    "        shap_values_label = shap_values[0][idx]\n",
    "        shap_summary[label] = shap_values_label.tolist()\n",
    "    \n",
    "    return {\"predictions\": result, \"shap_summary\": shap_summary}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "<class 'list'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Tokenizer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predict_text(\u001b[39m\"\u001b[39;49m\u001b[39mconstruction of road\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[69], line 13\u001b[0m, in \u001b[0;36mpredict_text\u001b[1;34m(sentence, threshold)\u001b[0m\n\u001b[0;32m     10\u001b[0m input_data_padded \u001b[39m=\u001b[39m pad_sequences(input_data, maxlen\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[39m# Calculate SHAP values\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m shap_values \u001b[39m=\u001b[39m explainer(input_data_padded)\n\u001b[0;32m     15\u001b[0m \u001b[39m# Summarize SHAP values for each label\u001b[39;00m\n\u001b[0;32m     16\u001b[0m shap_summary \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\shap\\explainers\\_permutation.py:62\u001b[0m, in \u001b[0;36mPermutation.__init__.<locals>.Permutation.__call__\u001b[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, max_evals\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, main_effects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, error_bounds\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     61\u001b[0m              outputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, silent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m---> 62\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\n\u001b[0;32m     63\u001b[0m         \u001b[39m*\u001b[39;49margs, max_evals\u001b[39m=\u001b[39;49mmax_evals, main_effects\u001b[39m=\u001b[39;49mmain_effects, error_bounds\u001b[39m=\u001b[39;49merror_bounds,\n\u001b[0;32m     64\u001b[0m         batch_size\u001b[39m=\u001b[39;49mbatch_size, outputs\u001b[39m=\u001b[39;49moutputs, silent\u001b[39m=\u001b[39;49msilent\n\u001b[0;32m     65\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\shap\\explainers\\_permutation.py:76\u001b[0m, in \u001b[0;36mPermutation.__call__\u001b[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, max_evals\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m, main_effects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, error_bounds\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, batch_size\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     73\u001b[0m              outputs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, silent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m     74\u001b[0m     \u001b[39m\"\"\" Explain the output of the model on the given arguments.\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\n\u001b[0;32m     77\u001b[0m         \u001b[39m*\u001b[39;49margs, max_evals\u001b[39m=\u001b[39;49mmax_evals, main_effects\u001b[39m=\u001b[39;49mmain_effects, error_bounds\u001b[39m=\u001b[39;49merror_bounds, batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m     78\u001b[0m         outputs\u001b[39m=\u001b[39;49moutputs, silent\u001b[39m=\u001b[39;49msilent\n\u001b[0;32m     79\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\shap\\explainers\\_explainer.py:264\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *args, **kwargs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     feature_names \u001b[39m=\u001b[39m [[] \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(args))]\n\u001b[0;32m    263\u001b[0m \u001b[39mfor\u001b[39;00m row_args \u001b[39min\u001b[39;00m show_progress(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39margs), num_rows, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m explainer\u001b[39m\u001b[39m\"\u001b[39m, silent):\n\u001b[1;32m--> 264\u001b[0m     row_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexplain_row(\n\u001b[0;32m    265\u001b[0m         \u001b[39m*\u001b[39mrow_args, max_evals\u001b[39m=\u001b[39mmax_evals, main_effects\u001b[39m=\u001b[39mmain_effects, error_bounds\u001b[39m=\u001b[39merror_bounds,\n\u001b[0;32m    266\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size, outputs\u001b[39m=\u001b[39moutputs, silent\u001b[39m=\u001b[39msilent, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    267\u001b[0m     )\n\u001b[0;32m    268\u001b[0m     values\u001b[39m.\u001b[39mappend(row_result\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mvalues\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    269\u001b[0m     output_indices\u001b[39m.\u001b[39mappend(row_result\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39moutput_indices\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\shap\\explainers\\_permutation.py:134\u001b[0m, in \u001b[0;36mPermutation.explain_row\u001b[1;34m(self, max_evals, main_effects, error_bounds, batch_size, outputs, silent, *row_args)\u001b[0m\n\u001b[0;32m    131\u001b[0m     i \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    133\u001b[0m \u001b[39m# evaluate the masked model\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m outputs \u001b[39m=\u001b[39m fm(masks, zero_index\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mbatch_size)\n\u001b[0;32m    136\u001b[0m \u001b[39mif\u001b[39;00m row_values \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     row_values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(fm),) \u001b[39m+\u001b[39m outputs\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m:])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\shap\\utils\\_masked_model.py:66\u001b[0m, in \u001b[0;36mMaskedModel.__call__\u001b[1;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[0;32m     64\u001b[0m         full_masks \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mint\u001b[39m(np\u001b[39m.\u001b[39msum(masks \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_masker_cols), dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[0;32m     65\u001b[0m         _convert_delta_mask_to_full(masks, full_masks)\n\u001b[1;32m---> 66\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_full_masking_call(full_masks, zero_index\u001b[39m=\u001b[39;49mzero_index, batch_size\u001b[39m=\u001b[39;49mbatch_size)\n\u001b[0;32m     68\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_full_masking_call(masks, batch_size\u001b[39m=\u001b[39mbatch_size)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\shap\\utils\\_masked_model.py:95\u001b[0m, in \u001b[0;36mMaskedModel._full_masking_call\u001b[1;34m(self, masks, zero_index, batch_size)\u001b[0m\n\u001b[0;32m     93\u001b[0m     masked_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmasker(delta_ind, \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs)\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m     94\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 95\u001b[0m     masked_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmasker(mask, \u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs)\n\u001b[0;32m     97\u001b[0m \u001b[39m# get a copy that won't get overwritten by the next iteration\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmasker, \u001b[39m\"\u001b[39m\u001b[39mimmutable_outputs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Tokenizer' object is not callable"
     ]
    }
   ],
   "source": [
    "predict_text(\"construction of road\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
